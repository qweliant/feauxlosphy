{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TextDataset, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "import pathlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-07 07:21:58.376582: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-10-07 07:21:58.376605: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\training_args.py:330: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  warnings.warn(\n",
      "10/07/2020 07:22:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "10/07/2020 07:22:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='./output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs\\\\Oct07_07-22-01_LAPTOP-4AHPBNQQ', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False)\n",
      "C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\modeling_auto.py:781: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1322: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\trainer.py:265: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
      "  warnings.warn(\n",
      "\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Iteration:   0%|          | 0/92 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Iteration:   1%|1         | 1/92 [00:00<01:10,  1.28it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"run_lan_modeling.py\", line 283, in <module>\n",
      "    main()\n",
      "  File \"run_lan_modeling.py\", line 247, in main\n",
      "    trainer.train(model_path=model_path)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\trainer.py\", line 763, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\trainer.py\", line 1113, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\trainer.py\", line 1137, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\modeling_gpt2.py\", line 752, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\modeling_gpt2.py\", line 645, in forward\n",
      "    outputs = block(\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\modeling_gpt2.py\", line 285, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\modeling_gpt2.py\", line 235, in forward\n",
      "    attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions)\n",
      "  File \"C:\\Users\\qweli\\miniconda3\\envs\\feaux\\lib\\site-packages\\transformers\\modeling_gpt2.py\", line 162, in _attn\n",
      "    w = torch.matmul(q, k)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 6.00 GiB total capacity; 4.44 GiB already allocated; 21.63 MiB free; 4.59 GiB reserved in total by PyTorch)\n",
      "10/07/2020 09:42:06 - INFO - wandb.internal.internal -   Internal process exited\n",
      "\n",
      "Epoch:   0%|          | 0/1 [00:01<?, ?it/s]\n",
      "\n",
      "Iteration:   1%|1         | 1/92 [00:01<02:03,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR=\"./output\"\n",
    "TRAIN_FILE=\"__train__.txt\"\n",
    "VALID_FILE=\"__valid__.txt\"\n",
    "\n",
    "!python run_lan_modeling.py \\\n",
    "    --output_dir=$OUTPUT_DIR \\\n",
    "    --model_type=sshleifer/tiny-gpt2 \\\n",
    "    --model_name_or_path=sshleifer/tiny-gpt2 \\\n",
    "    --do_train \\\n",
    "    --train_data_file=$TRAIN_FILE \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=$VALID_FILE \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --line_by_line \\\n",
    "    --evaluate_during_training \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'run_generation.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "K=50\n",
    "!python run_generation.py \\\n",
    "--model_type gpt2 \\\n",
    "--model_name_or_path $OUTPUT_DIR \\\n",
    "--length 300 \\\n",
    "--prompt \"<BOS>\" \\\n",
    "--stop_token \"<EOS>\" \\\n",
    "--k $K \\\n",
    "--num_return_sequences 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem: I neeed more data to fine tune each model to the persons speech pattern. \n",
    "    \n",
    "    I will basically be creating NLP philosphy generator\n",
    "    \n",
    "    this will result in 24 fine tuned models that represent each person, similar to the twitter models- calculate space cost of models, calculate time to train\n",
    "    \n",
    "    there is a trade off present: if I do not care about space I can just create and save a bunch of differnet models\n",
    "                                  if i do care, I will, train, test, create conversation, and upload in one go\n",
    "    \n",
    "    the first option seems straightforward, but the time to train the models could be problamatic, \n",
    "        and the space issue means that i am pretty much dealing with a local script that i cannot deploy somewhere.\n",
    "        i would eventually like to deploy this to a server and merely have a docker container.is this airflow use case?\n",
    "    \n",
    "    the second option could be faster, but i need to train one model for the 1st speaker and for each subsequent person\n",
    "    a marriage between the two will use the straightforward double for loop and check for model for diff users. if it doesnt exist, \n",
    "        train on the data for person, run algorithm. \n",
    "    \n",
    "## Problem: I still need an algorithm to generate a dialog between speaker x and speaker y\n",
    "    this may incorporate different pipelines\n",
    "        ex: prompt -> generate text -> summarize?, NER?, question answer? -> prompt\n",
    "    \n",
    "    a harder question is how to model the sequence format such that it resembles dialog\n",
    "    \n",
    "    I can just generate summerize the first speakers quotes, generate text, summerize, use as prompt for 2nd speaker, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
